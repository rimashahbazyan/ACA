{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm up\n",
    "\n",
    "The code below showcases a convolutional network in Keras. It was designed to classify 100x100 rgb images into 10 classes.\n",
    "This network... quite frankly, it sucks. Can you guess what's the problem? Is there just one problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl (173.9 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.15.5-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 614 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Using cached grpcio-1.32.0-cp38-cp38-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "Processing /Users/rimashahbazyan/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73/wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Using cached tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Using cached tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.19.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.3-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.27.1-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rimashahbazyan/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: protobuf, gast, absl-py, google-pasta, grpcio, wrapt, astunparse, tensorboard-plugin-wit, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, opt-einsum, tensorflow-estimator, keras-preprocessing, flatbuffers, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.27.1 google-auth-oauthlib-0.4.3 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "import keras.initializers as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = keras.models.Sequential()\n",
    "\n",
    "net.add(L.InputLayer([100, 100, 3]))\n",
    "\n",
    "net.add(L.Conv2D(filters=512, kernel_size=(3, 3), \n",
    "                 kernel_initializer=init.zeros()))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.Conv2D(filters=128, kernel_size=(3, 3), \n",
    "                 kernel_initializer=init.zeros()))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.Conv2D(filters=32, kernel_size=(3, 3), \n",
    "                 kernel_initializer=init.zeros()))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.MaxPool2D(pool_size=(6, 6)))\n",
    "\n",
    "net.add(L.Conv2D(filters=8, kernel_size=(10, 10), \n",
    "                 kernel_initializer=init.RandomNormal(), padding='same'))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "\n",
    "net.add(L.Conv2D(filters=8, kernel_size=(10, 10), \n",
    "                 kernel_initializer=init.RandomNormal(), padding='same'))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.MaxPool2D(pool_size=(3, 3)))\n",
    "\n",
    "net.add(L.Flatten()) # convert 3d tensor to a vector of features\n",
    "\n",
    "net.add(L.Dense(units=512))\n",
    "net.add(L.Activation('softmax'))\n",
    "\n",
    "net.add(L.Dropout(rate=0.5))\n",
    "\n",
    "net.add(L.Dense(units=512))\n",
    "net.add(L.Activation('softmax'))\n",
    "\n",
    "net.add(L.Dense(units=10))\n",
    "net.add(L.Activation('sigmoid'))\n",
    "net.add(L.Dropout(rate=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Conv2D](https://keras.io/layers/convolutional/#conv2d) - performs convolution:\n",
    "    * filters: number of output channels;\n",
    "    * kernel_size: an integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window;\n",
    "    * padding: padding=\"same\" adds zero padding to the input, so that the output has the same width and height, padding='valid' performs convolution only in locations where kernel and the input fully overlap;\n",
    "    * activation: \"relu\", \"tanh\", etc.\n",
    "    * input_shape: shape of input.\n",
    "* [MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d) - performs 2D max pooling.\n",
    "* [Flatten](https://keras.io/layers/core/#flatten) - flattens the input, does not affect the batch size.\n",
    "* [Dense](https://keras.io/layers/core/#dense) - fully-connected layer.\n",
    "    * Activation - applies an activation function.\n",
    "* [LeakyReLU](https://keras.io/layers/advanced-activations/#leakyrelu) - applies leaky relu activation.\n",
    "* [Dropout](https://keras.io/layers/core/#dropout) - applies dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book of grudges\n",
    "* zero init for weights will cause symmetry effect\n",
    "* Too many filters for first 3x3 convolution - will lead to enormous matrix while there's just not enough relevant combinations of 3x3 images (overkill).\n",
    "* Usually the further you go, the more filters you need.\n",
    "* large filters (10x10 is generally a bad pactice, and you definitely need more than 10 of them\n",
    "* the second of 10x10 convolution gets 8x6x6 image as input, so it's technically unable to perform such convolution.\n",
    "* Softmax nonlinearity effectively makes only 1 or a few neurons from the entire layer to \"fire\", rendering 512-neuron layer almost useless. Softmax at the output layer is okay though\n",
    "* Dropout after probability prediciton is just lame. A few random classes get probability of 0, so your probabilities no longer sum to 1 and crossentropy goes -inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you have to train a new Convolutional Neural Network from scratch for the classification of images.\n",
    "\n",
    "1. For this we will use the Keras library.\n",
    "2. The aim is to achieve 99% accuracy (on validation/test set) the MNIST dataset http://yann.lecun.com/exdb/mnist/.\n",
    "3. We have provided a basic Keras implementation of a CNN.\n",
    "4. You are allowed to do whatever you want (except copy pasting) with the network as long as it is explained in your report.\n",
    "5. Feel free to change the architecture of the network as well as parameters (e.g. learning rate, kernel sizes, ...).\n",
    "6. You can try to guess parameters manually of you want, just make sure that it performs better than 99% on the validation set.\n",
    "7. Sketch the final network architecture in your report.\n",
    "8. Make sure you train the network on the GPU, otherwise it will be too slow.\n",
    "9. Explain the plots: learning curve, accuracy wrt epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = keras.models.Sequential()\n",
    "\n",
    "net.add(L.InputLayer([100, 100, 3]))\n",
    "\n",
    "net.add(L.Conv2D(filters=30, kernel_size=(5, 5), \n",
    "                 kernel_initializer=init.zeros()))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.Conv2D(filters=50, kernel_size=(5, 5), \n",
    "                 kernel_initializer=init.zeros()))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.Conv2D(filters=90, kernel_size=(5, 5), \n",
    "                 kernel_initializer=init.zeros()))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "net.add(L.Conv2D(filters=50, kernel_size=(3, 3), \n",
    "                 kernel_initializer=init.RandomNormal(), padding='same'))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "\n",
    "net.add(L.Conv2D(filters=30, kernel_size=(3, 3), \n",
    "                 kernel_initializer=init.RandomNormal(), padding='same'))\n",
    "net.add(L.Activation('relu'))\n",
    "\n",
    "net.add(L.MaxPool2D(pool_size=(3, 3)))\n",
    "\n",
    "net.add(L.Flatten()) # convert 3d tensor to a vector of features\n",
    "\n",
    "net.add(L.Dense(units=512))\n",
    "net.add(L.Activation('sigmoid'))\n",
    "\n",
    "net.add(L.Dropout(rate=0.5))\n",
    "\n",
    "net.add(L.Dense(units=512))\n",
    "net.add(L.Activation('sigmoid'))\n",
    "\n",
    "net.add(L.Dense(units=10))\n",
    "net.add(L.Activation('softmax'))\n",
    "net.add(L.Dropout(rate=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `tf.keras.datasets.cifar10.load_data()` to get the data\n",
    "* split to 70 - 30 train / val using `train_test_split`\n",
    "* normalize the input like $x_{\\text{norm}} = \\frac{x}{255} - 0.5$\n",
    "* We need to convert class labels to one-hot encoded vectors. Use `keras.utils.to_categorical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-320861ae6824>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-320861ae6824>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    y_train = ### YOUR CODE HERE\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# normalize inputs\n",
    "# convert class labels to one-hot encoded, should have shape (?, NUM_CLASSES)\n",
    "x_train = ### YOUR CODE HERE\n",
    "x_test = ### YOUR CODE HERE\n",
    "\n",
    "x_val = ### YOUR CODE HERE\n",
    "x_val = ### YOUR CODE HERE\n",
    "\n",
    "y_test = ### YOUR CODE HERE\n",
    "y_test = ### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
